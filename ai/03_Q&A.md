# Q&A

These questions are taken from a book called _Artificial Intelligence: A Modern Approach_. These are my answers to them.

## Chater I

* **Q:** Suppose we extend Evans’s ANALOGY program so that it can score 200 on a standard IQ test. Would we then have a program more intelligent than a human?
  * No, of course not. It won't even prove that he is intelligent. To be intelligent, it had to have a rational agent approach.

* **Q:** Are reflex actions (such as flinching from a hot stove) rational? Are they intelligent?
  * In my opinion, I wouldn't call them intelligent. Reflex actions are a side-effect _because_ we are intelligent, but I wouldn't call them rational _nor_ intelligent.

* **Q:** Could I be wrong about what I’m thinking?
  * A: Yes, you can. But it's not natural, it's driven out of a former experience. For example, if I have some smaller problem right now, it might become larger it in my head because of something worse that happened to me before (maybe even in a completely different area). In that situation, I'm wrong. I think that I think about that small problem, while I'm actually trying to lower the consequences of my bigger problem by solving that little problem. In that situation, what I am actually thinking about is that bigger problem, and that's why my smaller problem seems more important than it actually is.

* **Q:** Why would evolution tend to result in systems that act rationally? What goals are such systems designed to achieve?
  * This is a mindblowing question. I have zero ideas. I will make sure to find some answers.

* **Q:** Is AI a science, or is it engineering? Or neither or both?
  * It's both. We need some thinkers to think about all kinds of problems we could run into while trying to create an artificial brain, and we need engineers and programmers to turn that thoughts into action. We need both sides to be included in the process.

* **Q:** “Surely animals cannot be intelligent—they can do only what their genes tell them.” Is the latter statement true, and does it imply the former?
  * In my opinion, the latter statement is not true. There is much more going on in animals. And I do believe that we can archive _some_ kind of intelligence in animals if we train them properly. So, I think that animals _could_ be intelligent, depending on your choice of the artificial intelligence definition. But, if it was true, that animals can do only what their genes tell them, that would imply that animals cannot be intelligent.

## Chapter II

* **Q:** Which is better—a reckless life of highs and lows, or a safe but humdrum existence? Which is better—an economy where everyone lives in moderate poverty, or one in which some live in plenty while others are very poor?
  * I don't think that the first question implies the second one. I think that it is better to have a reckless life. You will get to your ultimate low-high low some time, but if you have a right mindset, you might use that pain to get to your highest peak.
  * On the other hand, I don't think that we should have plenty of poor people. Instead, we should work on making the world better and our primary profit should not be personal gain.

* For each of the following assertions, say whether it is true or false and support your answer.
  * **Q:** An agent that senses only partial information about the state cannot be perfectly rational.
    * If I understood the question correctly, then I'd say that this is _true_. To archive a perfect rationality, the agent has to know the state he's currently facing with.
  * **Q:** There exist task environments in which no pure reflex agent can behave rationally.
    * Of course. There are environments in which he _has_ to learn from his experience, regardless of what his "instincts" told him.
  * **Q:** There exists a task environment in which every agent is rational.
    * True. In _very_ simple environments, a simple "mindless" agent can be pretty rational in the same way as some advanced rational agent.
  * **Q:** The input to an agent program is the same as the input to the agent function.
    * False. The data could be processed before it gets to the rational agent input.
  * **Q:** Every agent function is implementable by some program/machine combination.
    * Sure it is.
  * **Q:** A perfectly rational poker-playing agent never loses.
    * False. In card games, this has a lot to do with luck. No matter how rational he is, he won't be able to predict the cards that he's going to get. Because of that, it is not possible for him to win in every single game.

* **Q:** Suppose we keep the agent program fixed but speed up the machine by a factor of two. Does that change the agent function?
  * Yes. He might draw conclusions quicker, and because of that, he might immediately act differently than he would've reacted if he needed more time to draw conclusions.
